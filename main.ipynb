{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir=r'E:\\lastone\\test'\n",
    "train_dir=r'E:\\lastone\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,345,607\n",
      "Trainable params: 2,345,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emotion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 665s 1s/step - loss: 1.8006 - accuracy: 0.2593 - val_loss: 1.7112 - val_accuracy: 0.3313\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 648s 1s/step - loss: 1.6278 - accuracy: 0.3644 - val_loss: 1.5357 - val_accuracy: 0.4167\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 607s 1s/step - loss: 1.5253 - accuracy: 0.4143 - val_loss: 1.4649 - val_accuracy: 0.4408\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 853s 2s/step - loss: 1.4542 - accuracy: 0.4423 - val_loss: 1.4023 - val_accuracy: 0.4658\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 589s 1s/step - loss: 1.3902 - accuracy: 0.4693 - val_loss: 1.3435 - val_accuracy: 0.4867\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 594s 1s/step - loss: 1.3363 - accuracy: 0.4923 - val_loss: 1.3265 - val_accuracy: 0.4989\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 556s 1s/step - loss: 1.2911 - accuracy: 0.5102 - val_loss: 1.2657 - val_accuracy: 0.5177\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 942s 2s/step - loss: 1.2528 - accuracy: 0.5285 - val_loss: 1.2753 - val_accuracy: 0.5170\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 609s 1s/step - loss: 1.2217 - accuracy: 0.5403 - val_loss: 1.2107 - val_accuracy: 0.5437\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 619s 1s/step - loss: 1.1870 - accuracy: 0.5544 - val_loss: 1.1865 - val_accuracy: 0.5550\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 905s 2s/step - loss: 1.1612 - accuracy: 0.5647 - val_loss: 1.1716 - val_accuracy: 0.5625\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 322s 717ms/step - loss: 1.1316 - accuracy: 0.5762 - val_loss: 1.1548 - val_accuracy: 0.5674\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 250s 557ms/step - loss: 1.1058 - accuracy: 0.5858 - val_loss: 1.1589 - val_accuracy: 0.5604\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 253s 565ms/step - loss: 1.0832 - accuracy: 0.5966 - val_loss: 1.1354 - val_accuracy: 0.5705\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 247s 550ms/step - loss: 1.0620 - accuracy: 0.6020 - val_loss: 1.1193 - val_accuracy: 0.5836\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 247s 550ms/step - loss: 1.0414 - accuracy: 0.6100 - val_loss: 1.1088 - val_accuracy: 0.5868\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 249s 556ms/step - loss: 1.0149 - accuracy: 0.6237 - val_loss: 1.1032 - val_accuracy: 0.5897\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 249s 556ms/step - loss: 0.9910 - accuracy: 0.6316 - val_loss: 1.0927 - val_accuracy: 0.5939\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 249s 555ms/step - loss: 0.9693 - accuracy: 0.6418 - val_loss: 1.0880 - val_accuracy: 0.5965\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 251s 560ms/step - loss: 0.9472 - accuracy: 0.6485 - val_loss: 1.0840 - val_accuracy: 0.5985\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 248s 553ms/step - loss: 0.9217 - accuracy: 0.6590 - val_loss: 1.0706 - val_accuracy: 0.6062\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 248s 554ms/step - loss: 0.9042 - accuracy: 0.6651 - val_loss: 1.0763 - val_accuracy: 0.6060\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 259s 579ms/step - loss: 0.8815 - accuracy: 0.6755 - val_loss: 1.0703 - val_accuracy: 0.6092\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 268s 598ms/step - loss: 0.8587 - accuracy: 0.6860 - val_loss: 1.0670 - val_accuracy: 0.6145\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 263s 588ms/step - loss: 0.8368 - accuracy: 0.6939 - val_loss: 1.0767 - val_accuracy: 0.6106\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 272s 607ms/step - loss: 0.8118 - accuracy: 0.7021 - val_loss: 1.0712 - val_accuracy: 0.6141\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 274s 611ms/step - loss: 0.7914 - accuracy: 0.7078 - val_loss: 1.0604 - val_accuracy: 0.6205\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 272s 606ms/step - loss: 0.7682 - accuracy: 0.7205 - val_loss: 1.0747 - val_accuracy: 0.6184\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 273s 609ms/step - loss: 0.7417 - accuracy: 0.7290 - val_loss: 1.0641 - val_accuracy: 0.6219\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 273s 610ms/step - loss: 0.7302 - accuracy: 0.7353 - val_loss: 1.0650 - val_accuracy: 0.6244\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 2720s 6s/step - loss: 0.7073 - accuracy: 0.7416 - val_loss: 1.0709 - val_accuracy: 0.6208\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 899s 2s/step - loss: 0.6875 - accuracy: 0.7490 - val_loss: 1.0691 - val_accuracy: 0.6249\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 637s 1s/step - loss: 0.6632 - accuracy: 0.7584 - val_loss: 1.0669 - val_accuracy: 0.6257\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 282s 630ms/step - loss: 0.6376 - accuracy: 0.7688 - val_loss: 1.0709 - val_accuracy: 0.6297\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 308s 688ms/step - loss: 0.6203 - accuracy: 0.7742 - val_loss: 1.0780 - val_accuracy: 0.6288\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 331s 739ms/step - loss: 0.6032 - accuracy: 0.7829 - val_loss: 1.0823 - val_accuracy: 0.6323\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 339s 756ms/step - loss: 0.5786 - accuracy: 0.7920 - val_loss: 1.0870 - val_accuracy: 0.6303\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 329s 735ms/step - loss: 0.5569 - accuracy: 0.7997 - val_loss: 1.0915 - val_accuracy: 0.6316\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 14207s 32s/step - loss: 0.5407 - accuracy: 0.8051 - val_loss: 1.0891 - val_accuracy: 0.6325\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 761s 2s/step - loss: 0.5237 - accuracy: 0.8110 - val_loss: 1.1059 - val_accuracy: 0.6286\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 252s 563ms/step - loss: 0.5049 - accuracy: 0.8171 - val_loss: 1.1315 - val_accuracy: 0.6316\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 265s 591ms/step - loss: 0.4960 - accuracy: 0.8235 - val_loss: 1.1070 - val_accuracy: 0.6323\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 253s 565ms/step - loss: 0.4696 - accuracy: 0.8327 - val_loss: 1.1355 - val_accuracy: 0.6306\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 257s 574ms/step - loss: 0.4579 - accuracy: 0.8349 - val_loss: 1.1216 - val_accuracy: 0.6366\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 267s 597ms/step - loss: 0.4452 - accuracy: 0.8383 - val_loss: 1.1395 - val_accuracy: 0.6352\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 252s 562ms/step - loss: 0.4295 - accuracy: 0.8463 - val_loss: 1.1440 - val_accuracy: 0.6339\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 262s 584ms/step - loss: 0.4173 - accuracy: 0.8483 - val_loss: 1.1649 - val_accuracy: 0.6331\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 255s 570ms/step - loss: 0.4002 - accuracy: 0.8557 - val_loss: 1.1674 - val_accuracy: 0.6381\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 245s 548ms/step - loss: 0.3913 - accuracy: 0.8578 - val_loss: 1.1684 - val_accuracy: 0.6306\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 245s 548ms/step - loss: 0.3793 - accuracy: 0.8645 - val_loss: 1.1980 - val_accuracy: 0.6325\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# start the webcam feed\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "# pass here your video path\n",
    "# you may download one from here : https://www.pexels.com/video/three-girls-laughing-5273028/\n",
    "cap = cv2.VideoCapture(\"C:\\\\JustDoIt\\\\ML\\\\Sample_videos\\\\emotion_sample6.mp4\")\n",
    "\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
